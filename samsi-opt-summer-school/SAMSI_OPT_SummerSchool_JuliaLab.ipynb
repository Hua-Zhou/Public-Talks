{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\ba}{\\boldsymbol{a}}\n",
    "\\newcommand{\\bb}{\\boldsymbol{b}}\n",
    "\\newcommand{\\bc}{\\boldsymbol{c}}\n",
    "\\newcommand{\\bd}{\\boldsymbol{d}}\n",
    "\\newcommand{\\be}{\\boldsymbol{e}}\n",
    "\\newcommand{\\bff}{\\boldsymbol{f}}\n",
    "\\newcommand{\\bg}{\\boldsymbol{g}}\n",
    "\\newcommand{\\bh}{\\boldsymbol{h}}\n",
    "\\newcommand{\\bi}{\\boldsymbol{i}}\n",
    "\\newcommand{\\bj}{\\boldsymbol{j}}\n",
    "\\newcommand{\\bk}{\\boldsymbol{k}}\n",
    "\\newcommand{\\bl}{\\boldsymbol{l}}\n",
    "\\newcommand{\\bm}{\\boldsymbol{m}}\n",
    "\\newcommand{\\bn}{\\boldsymbol{n}}\n",
    "\\newcommand{\\bo}{\\boldsymbol{o}}\n",
    "\\newcommand{\\bp}{\\boldsymbol{p}}\n",
    "\\newcommand{\\bq}{\\boldsymbol{q}}\n",
    "\\newcommand{\\br}{\\boldsymbol{r}}\n",
    "\\newcommand{\\bs}{\\boldsymbol{s}}\n",
    "\\newcommand{\\bt}{\\boldsymbol{t}}\n",
    "\\newcommand{\\bu}{\\boldsymbol{u}}\n",
    "\\newcommand{\\bv}{\\boldsymbol{v}}\n",
    "\\newcommand{\\bw}{\\boldsymbol{w}}\n",
    "\\newcommand{\\bx}{\\boldsymbol{x}}\n",
    "\\newcommand{\\by}{\\boldsymbol{y}}\n",
    "\\newcommand{\\bz}{\\boldsymbol{z}}\n",
    "\\newcommand{\\bA}{\\boldsymbol{A}}\n",
    "\\newcommand{\\bB}{\\boldsymbol{B}}\n",
    "\\newcommand{\\bC}{\\boldsymbol{C}}\n",
    "\\newcommand{\\bD}{\\boldsymbol{D}}\n",
    "\\newcommand{\\bE}{\\boldsymbol{E}}\n",
    "\\newcommand{\\bF}{\\boldsymbol{F}}\n",
    "\\newcommand{\\bG}{\\boldsymbol{G}}\n",
    "\\newcommand{\\bH}{\\boldsymbol{H}}\n",
    "\\newcommand{\\bI}{\\boldsymbol{I}}\n",
    "\\newcommand{\\bJ}{\\boldsymbol{J}}\n",
    "\\newcommand{\\bK}{\\boldsymbol{K}}\n",
    "\\newcommand{\\bL}{\\boldsymbol{L}}\n",
    "\\newcommand{\\bM}{\\boldsymbol{M}}\n",
    "\\newcommand{\\bN}{\\boldsymbol{N}}\n",
    "\\newcommand{\\bO}{\\boldsymbol{O}}\n",
    "\\newcommand{\\bP}{\\boldsymbol{P}}\n",
    "\\newcommand{\\bQ}{\\boldsymbol{Q}}\n",
    "\\newcommand{\\bR}{\\boldsymbol{R}}\n",
    "\\newcommand{\\bS}{\\boldsymbol{S}}\n",
    "\\newcommand{\\bT}{\\boldsymbol{T}}\n",
    "\\newcommand{\\bU}{\\boldsymbol{U}}\n",
    "\\newcommand{\\bV}{\\boldsymbol{V}}\n",
    "\\newcommand{\\bW}{\\boldsymbol{W}}\n",
    "\\newcommand{\\bX}{\\boldsymbol{X}}\n",
    "\\newcommand{\\bY}{\\boldsymbol{Y}}\n",
    "\\newcommand{\\bZ}{\\boldsymbol{Z}}\n",
    "\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\bgamma}{\\boldsymbol{\\gamma}}\n",
    "\\newcommand{\\bdelta}{\\boldsymbol{\\delta}}\n",
    "\\newcommand{\\bepsilon}{\\boldsymbol{\\epsilon}}\n",
    "\\newcommand{\\blambda}{\\boldsymbol{\\lambda}}\n",
    "\\newcommand{\\bmu}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\bnu}{\\boldsymbol{\\nu}}\n",
    "\\newcommand{\\bphi}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\bpi}{\\boldsymbol{\\pi}}\n",
    "\\newcommand{\\bsigma}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\btheta}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\bomega}{\\boldsymbol{\\omega}}\n",
    "\\newcommand{\\bxi}{\\boldsymbol{\\xi}}\n",
    "\\newcommand{\\bGamma}{\\boldsymbol{\\Gamma}}\n",
    "\\newcommand{\\bDelta}{\\boldsymbol{\\Delta}}\n",
    "\\newcommand{\\bTheta}{\\boldsymbol{\\Theta}}\n",
    "\\newcommand{\\bLambda}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\bXi}{\\boldsymbol{\\Xi}}\n",
    "\\newcommand{\\bPi}{\\boldsymbol{\\Pi}}\n",
    "\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\bUpsilon}{\\boldsymbol{\\Upsilon}}\n",
    "\\newcommand{\\bPhi}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\bPsi}{\\boldsymbol{\\Psi}}\n",
    "\\newcommand{\\bOmega}{\\boldsymbol{\\Omega}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization in Julia\n",
    "##### Lab session for SAMSI Opt Program Summer School\n",
    "##### [Dr. Hua Zhou](mailto: huazhou@ucla.edu), UCLA\n",
    "##### Aug 10, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Download `Julia` from [http://julialang.org/downloads/](http://julialang.org/downloads/) and install on your computer.  \n",
    "0. In `Julia` command line, install packages needed for this tutorial by running following commands.\n",
    "<pre>\n",
    "Pkg.add(\"Distributions\")\n",
    "Pkg.add(\"Roots\")\n",
    "Pkg.add(\"RCall\")\n",
    "Pkg.add(\"Convex\")\n",
    "Pkg.add(\"CUBLAS\")\n",
    "Pkg.add(\"SCS\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"Gadfly\")\n",
    "Pkg.add(\"Images\")\n",
    "Pkg.add(\"JuMP\")\n",
    "Pkg.add(\"Ipopt\")\n",
    "</pre>\n",
    "0. Follow the instructions at [https://github.com/JuliaLang/IJulia.jl](https://github.com/JuliaLang/IJulia.jl) to install `IJulia.jl`, so that you can use Jupyter Notebook.  \n",
    "0. The tutorials *Hands-on Julia* by Dr. David P. Sanders, available at [https://github.com/dpsanders/hands_on_julia](https://github.com/dpsanders/hands_on_julia), are excellent. But they are **not** required for this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flowchart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Flowchart](./benchmark/optimization_flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Julia?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What features are we looking for in a language?  \n",
    "* Efficiency (in both run time and memory) for handling big data  \n",
    "* IDE support (debugging, profiling)\n",
    "* Open source  \n",
    "* Legacy code  \n",
    "* Tools for generating dynamic report (reproducibility)  \n",
    "* Adaptivity to hardware evolution (parallel and distributed computing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of language:  \n",
    "* Compiled languages: C, C++, Fortran  \n",
    "    * Directly compiled to machine code that is executed by CPU \n",
    "    * Pros: fast, memory efficient  \n",
    "    * Cons: longer development time, hard to debug\n",
    "* Interpreted languages: R, Matlab  \n",
    "    * Interpreted by interpreter  \n",
    "    * Pros: fast prototyping  \n",
    "    * Cons: excruciatingly slow for loops  \n",
    "* Mixed (dynamic) languages: Julia, Matlab (JIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language features of R, Matlab and Julia\n",
    "\n",
    "|        Features       |             R            |     Matlab     |        Julia        |\n",
    "|:---------------------:|:------------------------:|:--------------:|:-------------------:|\n",
    "|      Open source      |           :+1:           |      :-1:      |         :+1:        |\n",
    "|          IDE          |    RStudio :+1: :+1:     | :+1: :+1: :+1: |    Atom+Juno :-1:   |\n",
    "|    Dynamic document   | RMarkdown :+1: :+1: :+1: |    :+1: :+1:   |   IJulia :+1: :+1:  |\n",
    "|    Multi-threading    |    `parallel` package    |      :+1:      |    v0.5 :+1: :+1:   |\n",
    "|          JIT          |    `compiler` package    |    :+1: :+1:   |    :+1: :+1: :+1:   |\n",
    "|     Call C/Fortran    |      wrapper, `Rcpp`     |     wrapper    | no glue code needed |\n",
    "|  Call shared library  |          wrapper         |     wrapper    | no glue code needed |\n",
    "|         Typing        |           :-1:           |    :+1: :+1:   |    :+1: :+1: :+1:   |\n",
    "|   Pass by reference   |           :-1:           |      :-1:      |    :+1: :+1: :+1:   |\n",
    "|     Linear algebra    |           :-1:           |   MKL, Arpack  |  OpenBLAS, eigpack  |\n",
    "| Distributed computing |           :-1:           |      :+1:      |    :+1: :+1: :+1:   |\n",
    "| Sparse linear algebra |  `Matrix` package :-1:   | :+1: :+1: :+1: |    :+1: :+1: :+1:   |\n",
    "|     Documentation     |           :-1:           | :+1: :+1: :+1: |      :+1: :+1:      |\n",
    "|        Profiler       |           :-1:           | :+1: :+1: :+1: |    :+1: :+1: :+1:   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark code `R-benchmark-25.R` from [http://r.research.att.com/benchmarks/R-benchmark-25.R](http://r.research.att.com/benchmarks/R-benchmark-25.R) covers many commonly used numerical operations used in statistics. We ported (literally) to [Matlab](./benchmark/benchmark_matlab.m) and [Julia](./benchmark/benchmark_julia.jl) and report the run times (averaged over 5 runs) here.\n",
    "\n",
    "|                        Test                        | R 3.2.2 | Matlab R2014a | Julia 0.4.6 |\n",
    "|:-------------------------------------------------- |:-------:|:-------------:|:-----------:|\n",
    "| Matrix creation, trans., deform. (2500 x 2500) |   0.77  |      0.17     |   **0.14**  |\n",
    "|      Power of matrix (2500 x 2500, `A.^1000`)      |   0.26  |    **0.11**   |     0.22    |\n",
    "|          Quick sort ($n = 7 \\times 10^6$)          |   0.76  |    **0.24**   |     0.69    |\n",
    "|         Cross product (2800 x 2800, $A^TA$)        |  10.72  |      0.35     |   **0.31**  |\n",
    "|            LS solution ($n = p = 2000$)            |   1.28  |    **0.07**   |     0.09    |\n",
    "|                FFT ($n = 2,400,000$)               |   0.40  |    **0.04**   |     0.60    |\n",
    "|           Eigen-values ($600 \\times 600$)          |   0.82  |    **0.31**   |     0.36    |\n",
    "|          Determinant ($2500 \\times 2500$)          |   3.76  |      0.18     |   **0.16**  |\n",
    "|            Cholesky ($3000 \\times 3000$)           |   4.23  |    **0.15**   |     0.16    |\n",
    "|         Matrix inverse ($1600 \\times 1600$)        |   3.37  |    **0.16**   |     0.20    |\n",
    "|           Fibonacci (vector calculation)           |   0.34  |    **0.17**   |     0.68    |\n",
    "|            Hilbert (matrix calculation)            |   0.21  |      0.07     |   **0.01**  |\n",
    "|                   GCD (recursion)                  |   0.31  |      0.14     |   **0.12**  |\n",
    "|               Toeplitz matrix (loops)              |   0.36  |     0.0014    |  **0.0005** |\n",
    "|                 Escoufiers (mixed)                 |   0.45  |      0.40     |   **0.21**  |\n",
    "\n",
    "Machine specs: Intel i7 @ 2.6GHz (4 physical cores, 8 threads), 16G RAM, Mac OS 10.11.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gibbs sampler example taken from Doug Bates’s slides.\n",
    "> As some of you may know, I have had a (rather late) mid-life crisis and run off with another language called Julia.\n",
    "\n",
    "[Link to Jupyter Notebook](./benchmark/gibbs_julia.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE of Gamma distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use MLE of Gamma distribution to illustrate some rudiments of `Julia` programming. \n",
    "\n",
    "Let $x_1,\\ldots,x_m$ be a random sample from the gamma density\n",
    "\\begin{eqnarray*}\n",
    "f(x) & = & \\Gamma(\\alpha)^{-1} \\beta^{\\alpha} x^{\\alpha-1} e^{-\\beta x}\n",
    "\\end{eqnarray*}\n",
    "on $(0,\\infty)$. The loglikelihood function is\n",
    "\\begin{eqnarray*}\n",
    "    L(\\alpha, \\beta) = m [- \\ln \\Gamma(\\alpha) + \\alpha \\ln \\beta + (\\alpha - 1)\\overline{\\ln x} - \\beta \\bar x],\n",
    "\\end{eqnarray*}\n",
    "where $\\overline{x} = \\frac{1}{m} \\sum_{i=1}^m x_i$ and \n",
    "$\\overline{\\ln x} = \\frac{1}{m} \\sum_{i=1}^m \\ln x_i$. Following function \n",
    "evaluates the log-pdf of one data point `x` at parameter `α` and `β`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gamma_logpdf (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_logpdf(x::Real, α::Real, β::Real) = \n",
    "    - lgamma(α) + α * log(β) + (α - 1) * log(x) - β * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of `Julia`’s other strengths is its ability to infer the types of passed variables. To achieve the best computational efficiency, this information can be directly supplied by type annotation. Thus, the declaration `x::Real, α::Real, β::Real` alerts `Julia` to the fact that the data `x` and the parameters `α` and `β` must be a subtype of the abstract type `Real`. Mastery of type system allows one tap into `Julia`'s powerful multiple dispatch mechanism that dynamically dispatches functions according to the types of all arguments.\n",
    "\n",
    "`Julia` possesses a rich set of base functions. Our displayed code invokes the traditional scalar functions `log` and `lgamma`. Scalar functions can also act on vector arguments entry by entry. For example, if `x` is a vector, then `log(x)` delivers the natural logarithm of each entry of `x`. `Julia` documentation has a more complete [list](http://docs.julialang.org/en/release-0.4/manual/arrays/?highlight=vectorize#vectorized-operators-and-functions) of vectorized functions and operators. Scalar function not in the list can be easily vectorized. For example, to evaluate pdf or logpdf at a vector of observations, we can simply use the [`comprehension`](http://docs.julialang.org/en/release-0.4/manual/arrays/#comprehensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Any,1}:\n",
       " -0.768448\n",
       " -0.940515\n",
       " -0.673959\n",
       " -0.395453\n",
       " -0.313244"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srand(123)\n",
    "x = rand(5)\n",
    "[gamma_logpdf(xi, 1.0, 1.0) for xi in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or [`map`](http://docs.julialang.org/en/release-0.4/stdlib/collections/#Base.map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " -0.768448\n",
       " -0.940515\n",
       " -0.673959\n",
       " -0.395453\n",
       " -0.313244"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(xi -> gamma_logpdf(xi, 1.0, 1.0), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduction functions such as `sum`, `mean` and `norm` also act on vector arguments but return a scalar. `Julia` provides several convenient ways to construct reduction function. To evaluate the log-likelihood of a random sample, we can use `sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0916184386224517"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_logpdf(x::Vector, α::Real, β::Real) = \n",
    "    sum(xi -> gamma_logpdf(xi, α, β), x)\n",
    "gamma_logpdf(x, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or the more generic `mapreduce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0916184386224517"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_logpdf(x::Vector, α::Real, β::Real) = \n",
    "    mapreduce(xi -> gamma_logpdf(xi, α, β), +, x)\n",
    "gamma_logpdf(x, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "or the vecterized code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0916184386224517"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gamma_logpdf(x::Vector, α::Real, β::Real)\n",
    "    m = length(x)\n",
    "    avg = mean(x)\n",
    "    logavg = sum(log, x) / m\n",
    "    m * (- lgamma(α) + α * log(β) + (α - 1) * logavg - β * avg)\n",
    "end\n",
    "gamma_logpdf(x, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the function definition for evaluating pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gamma_pdf (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_pdf(x, α, β) = exp(gamma_logpdf(x, α, β))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "works for both scalar and vector input x because of the multiple dispatch of `gamma_logpdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46373237313439586"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_pdf(x[1], 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04542837184470604"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_pdf(x, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization often invokes taking derivatives of the objective function. The `ForwardDiff` package implements automatic differentiation. For example, to compute the derivative and Hessian of the log-likelihood with data `x` at `α=1.0` and `β=1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.0782854\n",
       " 1.90838  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ForwardDiff\n",
    "ForwardDiff.gradient(r -> gamma_logpdf(x, r...), [1.0; 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2x2 Array{Float64,2}:\n",
       " -8.22467   5.0\n",
       "  5.0      -5.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ForwardDiff.hessian(r -> gamma_logpdf(x, r...), [1.0; 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the score function equal to \n",
    "${\\bf 0}$ implies that $\\beta = \\alpha / \\bar x$ is a stationary\n",
    "point of the loglikelihood $L(\\alpha,\\beta)$ of the sample for \n",
    "$\\alpha$ fixed. In fact, $\\beta = \\alpha / \\bar x$ furnishes the maximum. \n",
    "Substituting this value of $\\beta$ in the loglikelihood reduces \n",
    "maximum likelihood estimation to optimization of the profile loglikelihood\n",
    "\\begin{eqnarray}\n",
    "L(\\alpha) = m [\\alpha \\ln \\alpha - \\alpha \\ln \\overline{x} - \\ln \\Gamma(\\alpha) + (\\alpha-1) \\overline{\\ln x} - \\alpha].\n",
    "\\end{eqnarray}\n",
    "The stationarity equation\n",
    "\\begin{eqnarray}\n",
    "0 = m \\left( \\ln \\alpha -  \\ln \\overline{x} -  \\psi(\\alpha) + \\overline{\\ln x} \\, \\right)\n",
    "\\end{eqnarray}\n",
    "is ripe for solution by Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mle_gamma (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mle_gamma(x::Vector)\n",
    "    avg = mean(x)\n",
    "    d = log(avg) - sum(log, x) / length(x)\n",
    "    α = fzero(r -> log(r) - digamma(r) - d, 1e-6, Inf)\n",
    "    β = α / avg\n",
    "    return (α, β)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a test example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameter values:\n",
      "α = 3.8422383759828493, β = 4.7025750035759355\n",
      "Estimator by MLE:\n",
      "α̂ = 3.646663250650827, β̂ = 4.859696892325012\n",
      "L(α̂, β̂) = -3548.969909540808\n",
      "Estimator by MLE (Distribution package):\n",
      "Distributions.Gamma{Float64}(α=3.6466632506507755, θ=4.859696892325092)\n"
     ]
    }
   ],
   "source": [
    "using Distributions, Roots\n",
    "srand(123)\n",
    "(n, p) = (1000, 2)\n",
    "(α, β) = 5.0 * rand(p)\n",
    "println(\"True parameter values:\")\n",
    "println(\"α = \", α, \", β = \", β)\n",
    "x = rand(Gamma(α, β), n)\n",
    "println(\"Estimator by MLE:\")\n",
    "(α, β) = mle_gamma(x)\n",
    "println(\"α̂ = \", α, \", β̂ = \", 1.0 / β)\n",
    "println(\"L(α̂, β̂) = \", gamma_logpdf(x, α, β))\n",
    "println(\"Estimator by MLE (Distribution package):\")\n",
    "println(fit_mle(Gamma, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try to use a generic nonlinear solver to find the MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max (nonlinear expression)\n",
      "Subject to\n",
      " α ≥ 1.0e-6\n",
      " β ≥ 1.0e-6\n",
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit http://projects.coin-or.org/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "This is Ipopt version 3.12.4, running with linear solver mumps.\n",
      "NOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:        0\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Total number of variables............................:        2\n",
      "                     variables with only lower bounds:        2\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        0\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  7.5268087e+03 0.00e+00 1.01e+00   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  6.7312928e+03 0.00e+00 2.28e-02  -1.6 2.29e-02    -  1.00e+00 1.00e+00f  1\n",
      "   2  6.5428048e+03 0.00e+00 2.53e-03  -3.9 9.24e-03    -  1.00e+00 1.00e+00f  1\n",
      "   3  5.7635102e+03 0.00e+00 1.35e-03  -5.3 2.81e-02    -  1.00e+00 1.00e+00f  1\n",
      "   4  5.4933325e+03 0.00e+00 1.04e-03  -6.7 3.69e-02    -  1.00e+00 3.71e-01f  2\n",
      "   5  5.0209025e+03 0.00e+00 4.94e-04  -7.2 6.51e-02    -  1.00e+00 1.00e+00f  1\n",
      "   6  5.0415228e+03 0.00e+00 1.33e-03  -4.9 2.09e-01    -  1.00e+00 1.00e+00f  1\n",
      "   7  4.6897552e+03 0.00e+00 1.01e-03  -4.9 4.28e-02    -  1.00e+00 1.00e+00f  1\n",
      "   8  4.2657944e+03 0.00e+00 1.43e-04  -5.3 1.90e-01    -  1.00e+00 3.77e-01f  2\n",
      "   9  4.1707516e+03 0.00e+00 6.99e-04  -7.4 9.99e-02    -  1.00e+00 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10  3.9901442e+03 0.00e+00 1.97e-04  -5.8 2.11e-01    -  1.00e+00 1.00e+00f  1\n",
      "  11  3.8142105e+03 0.00e+00 1.29e-04  -5.9 3.78e-01    -  1.00e+00 1.00e+00f  1\n",
      "  12  3.6881257e+03 0.00e+00 6.50e-05  -6.3 4.85e-01    -  1.00e+00 1.00e+00f  1\n",
      "  13  3.6052992e+03 0.00e+00 3.73e-05  -6.6 5.96e-01    -  1.00e+00 1.00e+00f  1\n",
      "  14  3.5644366e+03 0.00e+00 1.62e-05  -7.0 6.08e-01    -  1.00e+00 1.00e+00f  1\n",
      "  15  3.5511904e+03 0.00e+00 5.64e-06  -7.7 4.78e-01    -  1.00e+00 1.00e+00f  1\n",
      "  16  3.5493051e+03 0.00e+00 2.12e-05  -8.8 2.54e-01    -  1.00e+00 1.00e+00f  1\n",
      "  17  3.5490652e+03 0.00e+00 1.25e-05  -7.5 5.33e+01    -  1.00e+00 9.77e-04f 11\n",
      "  18  3.5489700e+03 0.00e+00 3.54e-07 -11.3 1.00e-02    -  1.00e+00 1.00e+00f  1\n",
      "  19  3.5489699e+03 0.00e+00 2.43e-09 -11.3 1.36e-03    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 19\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   3.5490071986838301e-04    3.5489699098311726e+03\n",
      "Dual infeasibility......:   2.4319353605387396e-09    2.4319098085817403e-02\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   5.0001944741929743e-12    5.0001419379470412e-05\n",
      "Overall NLP error.......:   2.4319353605387396e-09    2.4319098085817403e-02\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 44\n",
      "Number of objective gradient evaluations             = 20\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 0\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 0\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total CPU secs in IPOPT (w/o function evaluations)   =      0.077\n",
      "Total CPU secs in NLP function evaluations           =      0.026\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      "Objective value: -3548.9699098311726\n",
      "α = 3.6465449117377466\n",
      "β = 4.859861203560224\n"
     ]
    }
   ],
   "source": [
    "using JuMP, Ipopt, NLopt, KNITRO\n",
    "\n",
    "myf(a, b) = gamma_logpdf(x, a, b)\n",
    "JuMP.register(:myf, 2, myf, autodiff=true)\n",
    "\n",
    "m = Model(solver = IpoptSolver())\n",
    "#m = Model(solver = NLoptSolver(algorithm=:LD_MMA))\n",
    "#m = Model(solver = KnitroSolver())\n",
    "@variable(m, α >= 1e-6)\n",
    "@variable(m, β >= 1e-6)\n",
    "@NLobjective(m, Max, myf(α, β))\n",
    "\n",
    "print(m)\n",
    "status = solve(m)\n",
    "\n",
    "println(\"Objective value: \", getobjectivevalue(m))\n",
    "println(\"α = \", getvalue(α))\n",
    "println(\"β = \", 1 / getvalue(β))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE of Dirichlet distribution\n",
    "\n",
    "The Dirichlet distribution is used to model \n",
    "random proportions. It has probability density\n",
    "$$\n",
    "    \\frac{\\Gamma(\\theta_1 + \\cdots + \\theta_r)}{\\Gamma(\\theta_1) \\cdots \\Gamma(\\theta_r)} \\prod_{i=1}^r x_i^{\\theta_i - 1}\n",
    "$$\n",
    "on the unit simplex \n",
    "$\\{\\bx = (x_1,\\ldots,x_r)^t : x_1 > 0,\\ldots,x_r > 0 , \\sum_{i=1}^r x_i = 1 \\}$\n",
    "endowed with the uniform measure.  The Beta distribution is the\n",
    "special case $r=2$. \n",
    "\n",
    "If $\\bx_{1},\\ldots,\\bx_{m}$ are randomly sampled vectors from the \n",
    "Dirichlet distribution, then their loglikelihood is\n",
    "$$\n",
    "L(\\btheta) = m \\ln \\Gamma\\Big(\\sum_{i=1}^r \\theta_i\\Big) - m \\sum_{i=1}^r \\ln \\Gamma(\\theta_i) + \\sum_{j=1}^m \\sum_{i=1}^r (\\theta_i - 1) \\ln x_{ji}.\n",
    "$$\n",
    "Except for the first term on the right, the parameters are separated. Fortunately, the function $\\ln \\Gamma(t)$ is convex.  Its derivative, the\n",
    "digamma function, is denoted by $\\psi(t)$. The minorization\n",
    "$$\n",
    "\\ln \\Gamma\\Big(\\sum_{i=1}^r \\theta_i\\Big)\n",
    "\\ge\\ln \\Gamma\\Big(\\sum_{i=1}^r \\theta_{ni} \\Big) + \\psi\\Big(\\sum_{i=1}^r \\theta_{ni}\\Big) \\sum_{i=1}^r (\\theta_i-\\theta_{ni})\n",
    "$$\n",
    "generates the surrogate function\n",
    "$$\n",
    "g(\\btheta \\mid \\btheta_{n}) = m \\ln \\Gamma\\Big(\\sum_{i=1}^r \\theta_{ni}\\Big) + m \\psi\\Big(\\sum_{i=1}^r \\theta_{ni}\\Big) \\sum_{i=1}^r (\\theta_i-\\theta_{ni}) - m \\sum_{i=1}^r \\ln \\Gamma(\\theta_i) + \\sum_{j=1}^m \\sum_{i=1}^r (\\theta_i - 1) \\ln x_{ji}.\n",
    "$$\n",
    "Owing to the presence of the terms $\\ln \\Gamma(\\theta_i)$, the\n",
    "maximization step is analytically intractable.  However, Julia can readily solve\n",
    "the stationarity equation for each $\\theta_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mle_dirichlet (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mle_dirichlet(x::Matrix)\n",
    "    p = size(x, 1)\n",
    "    avglog = sum(log, x, 2) / size(x, 2)\n",
    "    (θ, old_θ) = (ones(p), ones(p))\n",
    "    for iteration = 1:500\n",
    "        c = digamma(sum(θ))\n",
    "        for i = 1:p\n",
    "            θ[i] = fzero(r -> avglog[i] + c - digamma(r), 1e-6, Inf)\n",
    "        end\n",
    "        if norm(θ - old_θ) < 1e-6\n",
    "            break\n",
    "        else\n",
    "            copy!(old_θ, θ)\n",
    "        end \n",
    "    end\n",
    "    return θ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameter value:\n",
      "θ = [3.3697934728403367,1.977265561675543,1.5662197790375931]\n",
      "MLE by Distribution package:\n",
      "θ̂ = [3.4645249501579634,2.1390802743658073,1.633669813514543]\n",
      "MLE by MM:\n",
      "θ̂ = [3.4645199552069186,2.139077459653193,1.6336678149454296]\n"
     ]
    }
   ],
   "source": [
    "(n, p) = (1000, 3)\n",
    "θ = 5.0rand(p)\n",
    "println(\"True parameter value:\")\n",
    "println(\"θ = \", θ)\n",
    "x = rand(Dirichlet(θ), n)\n",
    "println(\"MLE by Distribution package:\")\n",
    "println(\"θ̂ = \", fit_mle(Dirichlet, x).alpha)\n",
    "println(\"MLE by MM:\")\n",
    "θ = mle_dirichlet(x)\n",
    "println(\"θ̂ = \", θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNMF and GPU Computing\n",
    "\n",
    "In this example, we implement the MM algorithm for the Lee and Seung's Nonnegative Matrix Factorization (NNMF) algorithm and explore the GPU computing in Julia.\n",
    "\n",
    "Many EM/MM algorithms are amenable to massively parallel computing using GPU. See examples in the paper [*Graphics Processing Units and High-Dimensional Optimization*](http://projecteuclid.org/download/pdfview_1/euclid.ss/1294167962)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUs are ubiquitous in modern computers. Following are typical computer systems in **2013**.\n",
    "\n",
    "| NVIDIA GPUs         | Tesla M2090                            | GTX 580                                 | GT 650M                              |\n",
    "|---------------------|----------------------------------------|-----------------------------------------|--------------------------------------|\n",
    "|                     | ![Tesla M2090](./nnmf/tesla_m2090.jpg) | ![GTX 580](./nnmf/nvidia_gtx580.jpg)    | ![GT 650M](./nnmf/nvidia_gt650m.jpg) |\n",
    "| Computers           | servers, cluster                       | desktop                                 | laptop                               |\n",
    "|                     | ![Server](./nnmf/gpu_server.jpg)       | ![Desktop](./nnmf/alienware-area51.png) | ![Laptop](./nnmf/macpro_inside.png)  |\n",
    "| Main usage          | scientific computing                   | daily work, gaming                      | daily work                           |\n",
    "| Memory              | 6GB                                    | 1.5GB                                   | 1GB                                  |\n",
    "| Memory bandwidth    | 177GB/sec                              | 192GB/sec                               | 80GB/sec                             |\n",
    "| Number of cores     | 512                                    | 512                                     | 384                                  |\n",
    "| Processor clock     | 1.3GHz                                 | 1.5GHz                                  | 0.9GHz                               |\n",
    "| Peak DP performance | 666Gflops                              |                                         |                                      |\n",
    "| Peak SP performance | 13332Gflops                            | 1581Gflops                              | 691Gflops                            |\n",
    "| Release price       | \\$2500                                  | \\$500                                    | OEM                                  |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU architecture vs CPU architecture.  \n",
    "* GPUs contain 100s of processing cores on a single card; several cards can fit in a desktop PC  \n",
    "* Each core carries out the same operations in parallel on different input data -- single program, multiple data (SPMD) paradigm  \n",
    "* Extremely high arithmetic intensity *if* one can transfer the data onto and results off of the processors quickly\n",
    "\n",
    "| ![i7 die](./nnmf/cpu_i7_die.png) | ![Fermi die](./nnmf/Fermi_Die.png) |\n",
    "|----------------------------------|------------------------------------|\n",
    "| ![Einstein](./nnmf/einstein.png) | ![Rain man](./nnmf/rainman.png)    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to [Jupyter Notebook](./nnmf/nnmf.ipynb).\n",
    "\n",
    "Exercise: Re-implement the NNMF algorithm using the [ArrayFire.jl](https://github.com/JuliaComputing/ArrayFire.jl) package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCP Using Convex.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard convex problem classes like LP (linear programming), QP (quadratic programming), SOCP (second-order cone programming), SDP (semidefinite programming), and GP (geometric programming), are becoming a **technology**. I illustrate `Julia` solutions to some statistical applications.\n",
    "\n",
    "![DCP Hierarchy](./convex.jl/convex-hierarchy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like computer languages, getting familiar with **good** optimization softwares broadens the scope and scale of problems we are able to solve in statistics. Following table lists some of the best optimization softwares. Use of Gurobi and/or Mosek is highly recommended. \n",
    "\n",
    "|           |   | LP | MILP | SOCP |     MISOCP     | SDP | GP | NLP | MINLP |   | R | Matlab | Julia | Python |   | Cost |\n",
    "|:---------:|:-:|:--:|:----:|:----:|:--------------:|:---:|:--:|:---:|:-----:|:-:|:-:|:------:|:-----:|:------:|:-:|:----:|\n",
    "|   **modeling tools**   |   |    |      |      |                |     |    |     |       |   |   |        |       |        |   |      |\n",
    "|  JuMP.jl  |   |  x |   x  |   x  |        x       |     |    |  x  |   x   |   |   |        |   x   |        |   |   O  |\n",
    "| Convex.jl |   |  x |   x  |   x  |        x       |  x  |    |     |       |   |   |        |   x   |        |   |   O  |\n",
    "|    cvx    |   |  x |   x  |   x  |        x       |  x  |  x |     |       |   |   |    x   |       |    x   |   |   A  |\n",
    "|   **convex solvers** |   |    |      |      |                |     |    |     |       |   |   |        |       |        |   |      |\n",
    "|   Gurobi  |   |  x |   x  |   x  |        x       |     |    |     |       |   | x |    x   |   x   |    x   |   |   A  |\n",
    "|   Mosek   |   |  x |   x  |   x  |        x       |  x  |  x |  x  |       |   | x |    x   |   x   |    x   |   |   A  |\n",
    "|   CPLEX   |   |  x |   x  |   x  |        x       |     |    |     |       |   | ? |    x   |   x   |    x   |   |   A  |\n",
    "|    SCS    |   |  x |      |   x  |                |  x  |    |     |       |   |   |    x   |   x   |    x   |   |   O  |\n",
    "|   SeDuMi  |   |  x |      |   x  |                |  x  |  ? |     |       |   |   |    x   |       |        |   |   O  |\n",
    "|   SDPT3   |   |  x |      |   x  |                |  x  |  ? |     |       |   |   |    x   |       |        |   |   O  |\n",
    "|   **NLP solvers**  |   |    |      |      |                |     |    |     |       |   |   |        |       |        |   |      |\n",
    "|   KNITRO  |   |  x |   x  |      |                |     |    |  x  |   x   |   | x |    x   |   x   |    x   |   |   $  |\n",
    "|   NLopt   |   |  x |      |      |                |     |    |  x  |       |   |   |    x   |   x   |    x   |   |   O  |\n",
    "|   Ipopt   |   |  x |      |      |                |     |    |  x  |       |   |   |    x   |   x   |    x   |   |   O  |\n",
    "\n",
    "* O: open source  \n",
    "* A: free academic license  \n",
    "* $: commercial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to [Jupyter Notebook](./convex.jl/convex.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.6",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
