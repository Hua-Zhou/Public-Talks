{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\ba}{\\boldsymbol{a}}\n",
    "\\newcommand{\\bb}{\\boldsymbol{b}}\n",
    "\\newcommand{\\bc}{\\boldsymbol{c}}\n",
    "\\newcommand{\\bd}{\\boldsymbol{d}}\n",
    "\\newcommand{\\be}{\\boldsymbol{e}}\n",
    "\\newcommand{\\bff}{\\boldsymbol{f}}\n",
    "\\newcommand{\\bg}{\\boldsymbol{g}}\n",
    "\\newcommand{\\bh}{\\boldsymbol{h}}\n",
    "\\newcommand{\\bi}{\\boldsymbol{i}}\n",
    "\\newcommand{\\bj}{\\boldsymbol{j}}\n",
    "\\newcommand{\\bk}{\\boldsymbol{k}}\n",
    "\\newcommand{\\bl}{\\boldsymbol{l}}\n",
    "\\newcommand{\\bm}{\\boldsymbol{m}}\n",
    "\\newcommand{\\bn}{\\boldsymbol{n}}\n",
    "\\newcommand{\\bo}{\\boldsymbol{o}}\n",
    "\\newcommand{\\bp}{\\boldsymbol{p}}\n",
    "\\newcommand{\\bq}{\\boldsymbol{q}}\n",
    "\\newcommand{\\br}{\\boldsymbol{r}}\n",
    "\\newcommand{\\bs}{\\boldsymbol{s}}\n",
    "\\newcommand{\\bt}{\\boldsymbol{t}}\n",
    "\\newcommand{\\bu}{\\boldsymbol{u}}\n",
    "\\newcommand{\\bv}{\\boldsymbol{v}}\n",
    "\\newcommand{\\bw}{\\boldsymbol{w}}\n",
    "\\newcommand{\\bx}{\\boldsymbol{x}}\n",
    "\\newcommand{\\by}{\\boldsymbol{y}}\n",
    "\\newcommand{\\bz}{\\boldsymbol{z}}\n",
    "\\newcommand{\\bA}{\\boldsymbol{A}}\n",
    "\\newcommand{\\bB}{\\boldsymbol{B}}\n",
    "\\newcommand{\\bC}{\\boldsymbol{C}}\n",
    "\\newcommand{\\bD}{\\boldsymbol{D}}\n",
    "\\newcommand{\\bE}{\\boldsymbol{E}}\n",
    "\\newcommand{\\bF}{\\boldsymbol{F}}\n",
    "\\newcommand{\\bG}{\\boldsymbol{G}}\n",
    "\\newcommand{\\bH}{\\boldsymbol{H}}\n",
    "\\newcommand{\\bI}{\\boldsymbol{I}}\n",
    "\\newcommand{\\bJ}{\\boldsymbol{J}}\n",
    "\\newcommand{\\bK}{\\boldsymbol{K}}\n",
    "\\newcommand{\\bL}{\\boldsymbol{L}}\n",
    "\\newcommand{\\bM}{\\boldsymbol{M}}\n",
    "\\newcommand{\\bN}{\\boldsymbol{N}}\n",
    "\\newcommand{\\bO}{\\boldsymbol{O}}\n",
    "\\newcommand{\\bP}{\\boldsymbol{P}}\n",
    "\\newcommand{\\bQ}{\\boldsymbol{Q}}\n",
    "\\newcommand{\\bR}{\\boldsymbol{R}}\n",
    "\\newcommand{\\bS}{\\boldsymbol{S}}\n",
    "\\newcommand{\\bT}{\\boldsymbol{T}}\n",
    "\\newcommand{\\bU}{\\boldsymbol{U}}\n",
    "\\newcommand{\\bV}{\\boldsymbol{V}}\n",
    "\\newcommand{\\bW}{\\boldsymbol{W}}\n",
    "\\newcommand{\\bX}{\\boldsymbol{X}}\n",
    "\\newcommand{\\bY}{\\boldsymbol{Y}}\n",
    "\\newcommand{\\bZ}{\\boldsymbol{Z}}\n",
    "\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\bgamma}{\\boldsymbol{\\gamma}}\n",
    "\\newcommand{\\bdelta}{\\boldsymbol{\\delta}}\n",
    "\\newcommand{\\bepsilon}{\\boldsymbol{\\epsilon}}\n",
    "\\newcommand{\\blambda}{\\boldsymbol{\\lambda}}\n",
    "\\newcommand{\\bmu}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\bnu}{\\boldsymbol{\\nu}}\n",
    "\\newcommand{\\bphi}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\bpi}{\\boldsymbol{\\pi}}\n",
    "\\newcommand{\\bsigma}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\btheta}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\bomega}{\\boldsymbol{\\omega}}\n",
    "\\newcommand{\\bxi}{\\boldsymbol{\\xi}}\n",
    "\\newcommand{\\bGamma}{\\boldsymbol{\\Gamma}}\n",
    "\\newcommand{\\bDelta}{\\boldsymbol{\\Delta}}\n",
    "\\newcommand{\\bTheta}{\\boldsymbol{\\Theta}}\n",
    "\\newcommand{\\bLambda}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\bXi}{\\boldsymbol{\\Xi}}\n",
    "\\newcommand{\\bPi}{\\boldsymbol{\\Pi}}\n",
    "\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\bUpsilon}{\\boldsymbol{\\Upsilon}}\n",
    "\\newcommand{\\bPhi}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\bPsi}{\\boldsymbol{\\Psi}}\n",
    "\\newcommand{\\bOmega}{\\boldsymbol{\\Omega}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistical Computing Using Julia\n",
    "\n",
    "\n",
    "##### Short course at [2016 IISA Conference](http://blogs.oregonstate.edu/iisa/)\n",
    "##### Dr. Hua Zhou, [huazhou@ucla.edu](mailto: huazhou@ucla.edu)\n",
    "##### Department of Biostatistics, UCLA\n",
    "##### Aug 18, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course material\n",
    "\n",
    "Course material is available at [https://github.com/Hua-Zhou/Public-Talks/tree/master/iisa-conference-2016](https://github.com/Hua-Zhou/Public-Talks/tree/master/iisa-conference-2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preparation\n",
    "\n",
    "0. Download current release of `Julia` (v0.4.6) from [http://julialang.org/downloads/](http://julialang.org/downloads/) and install on your computer.  \n",
    "0. In `Julia` command line, install packages needed for this tutorial by running following commands.\n",
    "<pre>\n",
    "Pkg.add(\"Distributions\")\n",
    "Pkg.add(\"ForwardDiff\")\n",
    "Pkg.add(\"Roots\")\n",
    "Pkg.add(\"RCall\")\n",
    "Pkg.add(\"Convex\")\n",
    "Pkg.add(\"CUBLAS\") # requires Nvidia GPU and drivers\n",
    "Pkg.add(\"SCS\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"Gadfly\")\n",
    "Pkg.add(\"Images\")\n",
    "Pkg.add(\"JuMP\")\n",
    "Pkg.add(\"Ipopt\")\n",
    "</pre>\n",
    "0. Follow the instructions at [https://github.com/JuliaLang/IJulia.jl](https://github.com/JuliaLang/IJulia.jl) to install `IJulia.jl`, so that you can use Jupyter Notebook.  \n",
    "0. Read a cheat sheet by Ian Hellstrom, [*The Fast Track to Julia*](https://dl.dropboxusercontent.com/u/8252984/julia.html).\n",
    "0. The tutorials [*Hands-on Julia*](https://github.com/dpsanders/hands_on_julia) by Dr. David P. Sanders, available at [https://github.com/dpsanders/hands_on_julia](https://github.com/dpsanders/hands_on_julia), are excellent. But they are **not** required for this short course.  \n",
    "0. For `Matlab` users, read [*Noteworthy Differences From Matlab*](http://docs.julialang.org/en/release-0.4/manual/noteworthy-differences/#noteworthy-differences-from-matlab). For `R` users, read [*Noteworthy Differences From R*](http://docs.julialang.org/en/release-0.4/manual/noteworthy-differences/#noteworthy-differences-from-r)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to run examples in this tutorial\n",
    "\n",
    "There are at least two ways to run examples in this tutorial.  \n",
    "* Copy code into `Julia` REPL and run.  \n",
    "* Install [`IJulia` package](https://github.com/JuliaLang/IJulia.jl) or [`Jupyter`](http://jupyter.org) and run the notebook directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Julia?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What features are we looking for in a language?  \n",
    "* Efficiency (in both run time and memory) for handling big data  \n",
    "* IDE support (debugging, profiling)\n",
    "* Open source  \n",
    "* Legacy code  \n",
    "* Tools for generating dynamic report (reproducibility)  \n",
    "* Adaptivity to hardware evolution (parallel and distributed computing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of language \n",
    "\n",
    "* Compiled languages: C, C++, Fortran  \n",
    "    * Directly compiled to machine code that is executed by CPU \n",
    "    * Pros: fast, memory efficient  \n",
    "    * Cons: longer development time, hard to debug\n",
    "* Interpreted languages: R, Matlab, Python, SAS IML  \n",
    "    * Interpreted by interpreter  \n",
    "    * Pros: fast prototyping  \n",
    "    * Cons: excruciatingly slow for loops  \n",
    "* Mixed (dynamic) languages: Julia, Matlab (JIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Language features of R, Matlab and Julia\n",
    "\n",
    "|        Features       |             R            |     Matlab     |        Julia        |\n",
    "|:---------------------:|:------------------------:|:--------------:|:-------------------:|\n",
    "|      Open source      |           👍         |      👎      |         👍        |\n",
    "|          IDE          |    RStudio 👍 👍     | 👍 👍 👍 |    Atom+Juno 👎   |\n",
    "|    Dynamic document   | RMarkdown 👍 👍 👍 |    👍 👍   |   IJulia 👍 👍  |\n",
    "|    Multi-threading    |    `parallel` 👎  |      👍      |  👍 👍 [see docs](http://docs.julialang.org/en/release-0.4/manual/parallel-computing/)  |\n",
    "|          JIT          |    `compiler` 👎  |    👍 👍   |    👍 👍 👍   |\n",
    "|     Call C/Fortran    |      wrapper, `Rcpp`     |     wrapper    | [no glue code needed](http://docs.julialang.org/en/release-0.4/manual/calling-c-and-fortran-code/#calling-c-and-fortran-code) |\n",
    "|  Call shared library  |          wrapper         |     wrapper    | [no glue code needed](http://docs.julialang.org/en/release-0.4/manual/calling-c-and-fortran-code/#calling-c-and-fortran-code) |\n",
    "|         Typing        |           👎           |    👍 👍   |    👍 👍 👍   |\n",
    "|   Pass by reference   |           👎           |      👎      |    👍 👍 👍   |\n",
    "|     Linear algebra    |           👎           |   MKL, Arpack  |  OpenBLAS, eigpack  |\n",
    "| Distributed computing |           👎           |      👍      |    👍 👍 👍   |\n",
    "| Sparse linear algebra |  `Matrix` package 👎   | 👍 👍 👍 |    👍 👍 👍   |\n",
    "|     Documentation     |           👎           | 👍 👍 👍 |      👍 👍      |\n",
    "|        Profiler       |           👎           | 👍 👍 👍 |    👍 👍 👍   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Benchmark\n",
    "\n",
    "Benchmark code `R-benchmark-25.R` from [http://r.research.att.com/benchmarks/R-benchmark-25.R](http://r.research.att.com/benchmarks/R-benchmark-25.R) covers many commonly used numerical operations used in statistics. We ported (literally) to [Matlab](./benchmark/benchmark_matlab.m) and [Julia](./benchmark/benchmark_julia.jl) and report the run times (averaged over 5 runs) here.\n",
    "\n",
    "|                        Test                        | R 3.3.1 | Matlab R2014a | Julia 0.4.6 |\n",
    "|:-------------------------------------------------- |:-------:|:-------------:|:-----------:|\n",
    "| Matrix creation, trans., deform. (2500 x 2500) |   0.73  |      0.17     |   **0.14**  |\n",
    "|      Power of matrix (2500 x 2500, `A.^1000`)      |   0.25  |    **0.11**   |     0.22    |\n",
    "|          Quick sort ($n = 7 \\times 10^6$)          |   0.74  |    **0.24**   |     0.69    |\n",
    "|         Cross product (2800 x 2800, $A^TA$)        |  11.07  |      0.35     |   **0.31**  |\n",
    "|            LS solution ($n = p = 2000$)            |   1.27  |    **0.07**   |     0.09    |\n",
    "|                FFT ($n = 2,400,000$)               |   0.42  |    **0.04**   |     0.60    |\n",
    "|           Eigen-values ($600 \\times 600$)          |   0.86  |    **0.31**   |     0.36    |\n",
    "|          Determinant ($2500 \\times 2500$)          |   3.80  |      0.18     |   **0.16**  |\n",
    "|            Cholesky ($3000 \\times 3000$)           |   4.19  |    **0.15**   |     0.16    |\n",
    "|         Matrix inverse ($1600 \\times 1600$)        |   3.32  |    **0.16**   |     0.20    |\n",
    "|           Fibonacci (vector calculation)           |   0.33  |    **0.17**   |     0.68    |\n",
    "|            Hilbert (matrix calculation)            |   0.22  |      0.07     |   **0.01**  |\n",
    "|                   GCD (recursion)                  |   0.25  |      0.14     |   **0.12**  |\n",
    "|               Toeplitz matrix (loops)              |   0.40  |     0.0014    |  **0.0005** |\n",
    "|                 Escoufiers (mixed)                 |   0.41  |      0.40     |   **0.21**  |\n",
    "\n",
    "Machine specs: Intel i7 @ 2.6GHz (4 physical cores, 8 threads), 16G RAM, Mac OS 10.11.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs sampler example by Doug Bates\n",
    "\n",
    "Gibbs sampler example taken from Doug Bates’s slides\n",
    "> As some of you may know, I have had a (rather late) mid-life crisis and run off with another language called Julia.\n",
    "\n",
    "[Link to Jupyter Notebook](./benchmark/gibbs_julia.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brownian motion example by John Myles White\n",
    "\n",
    "This example is taken from John Myles White talk at [Multithreaded Data event](http://multithreaded.stitchfix.com/blog/2015/03/05/john-myles-white-on-julia/).\n",
    "\n",
    "[Link to Jupyter Notebook](./benchmark/brownianmotion_julia.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Julia REPL (Read-Evaluation-Print-Loop)\n",
    "\n",
    "The `Julia` REPL has four main modes.\n",
    "\n",
    "* Default mode is the Julian prompt `julia>`. Type backspace in other modes to enter defualt mode.    \n",
    "* Help mode `help?>`. Type `?` to enter help mode.  \n",
    "* Shell mode `shell>`. Type `;` to enter shell mode.  \n",
    "* Search mode `(reverse-i-search)`. Type `^R` to enter search model. \n",
    "\n",
    "With `RCall.jl` package installed, we can enter the `R` mode by typing `$` at Julia REPL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package system\n",
    "\n",
    "Each Julia package is a git repository. For example, the package called `Distributions.jl` is added with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pkg.add(\"Distributions\")   # no .jl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and \"removed\" (although not completely deleted) with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pkg.rm(\"Distributions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package manager actually provides a dependency solver that determines which packages are actually required to be installed.\n",
    "\n",
    "The package ecosystem is rapidly maturing; a complete list of registered packages (which are required to have a certain level of testing and documentation) is available at [http://pkg.julialang.org/](http://pkg.julialang.org/). Non-registered packages are added by cloning the relevant git repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pkg.clone(\"git@github.com:OpenMendel/SnpArrays.jl.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A package need only be added once, at which point it is downloaded into your local .julia directory in your home directory, in a subdirectory v0.3 or v0.4, depending on your Julia version. If you start having problems with packages that seem to be unsolvable, you can try just deleting your .julia directory and reinstalling all your packages. Periodically, you should run `Pkg.update()` which checks for, downloads and installs updated versions of all the packages you currently have installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pulls all of the *exported* functions in the module into your local namespace, as you can check using the `whos()` command. An alternative is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the functions from the `Distributions` package are available only using `Distributions.<NAME>`. All functions, not only exported functions, are always available like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which IDE (Integrated Development Environment)\n",
    "\n",
    "I'm using the editor [Atom](https://atom.io) with packages `julia-client`, `language-julia`, and `latex-completions` installed. \n",
    "\n",
    "If you want a RStudio- or Matlab- like IDE, add the `uber-juno` package to Atom. Follow instructions at [https://github.com/JunoLab/uber-juno/blob/master/setup.md](https://github.com/JunoLab/uber-juno/blob/master/setup.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple dispatch and methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `Julia`, different functions (methods) can have the same name. `Julia` operates on [multiple dispatch](http://docs.julialang.org/en/release-0.4/manual/methods/) to determine which method to use. That is `Julia` dynamically determines the method according to the type signature of input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "11 methods for generic function <b>sin</b>:<ul><li> sin(a::<b>Complex{Float16}</b>) at <a href=\"https://github.com/JuliaLang/julia/tree/2e358ce975029ec97aba5994c17d4a2169c3b085/base/float16.jl#L151\" target=\"_blank\">float16.jl:151</a><li> sin(z::<b>Complex{T<:Real}</b>) at <a href=\"https://github.com/JuliaLang/julia/tree/2e358ce975029ec97aba5994c17d4a2169c3b085/base/complex.jl#L548\" target=\"_blank\">complex.jl:548</a><li> sin(x::<b>Float64</b>) at <a href=\"https://github.com/JuliaLang/julia/tree/2e358ce975029ec97aba5994c17d4a2169c3b085/base/math.jl#L137\" target=\"_blank\">math.jl:137</a><li> sin(x::<b>Float32</b>) at <a href=\"https://github.com/JuliaLang/julia/tree/2e358ce975029ec97aba5994c17d4a2169c3b085/base/math.jl#L138\" target=\"_blank\">math.jl:138</a><li> sin(a::<b>Float16</b>) at <a href=\"https://github.com/JuliaLang/julia/tree/2e358ce975029ec97aba5994c17d4a2169c3b085/base/float16.jl#L150\" target=\"_blank\">float16.jl:150</a><li> sin(x::<b>BigFloat</b>) at <a href=\"https://github.com/JuliaLang/julia/tree/2e358ce975029ec97aba5994c17d4a2169c3b085/base/mpfr.jl#L613\" target=\"_blank\">mpfr.jl:613</a><li> sin(x::<b>Real</b>) at <a href=\"https://github.com/JuliaLang/julia/tree/2e358ce975029ec97aba5994c17d4a2169c3b085/base/math.jl#L139\" target=\"_blank\">math.jl:139</a><li> sin<i>{Tv,Ti}</i>(A::<b>SparseMatrixCSC{Tv,Ti}</b>) at <a href=\"https://github.com/JuliaLang/julia/tree/2e358ce975029ec97aba5994c17d4a2169c3b085/base/sparse/sparsematrix.jl#L648\" target=\"_blank\">sparse/sparsematrix.jl:648</a><li> sin<i>{T<:Number}</i>(::<b>AbstractArray{T<:Number,1}</b>) at <a href=\"https://github.com/JuliaLang/julia/tree/2e358ce975029ec97aba5994c17d4a2169c3b085/base/operators.jl#L380\" target=\"_blank\">operators.jl:380</a><li> sin<i>{T<:Number}</i>(::<b>AbstractArray{T<:Number,2}</b>) at <a href=\"https://github.com/JuliaLang/julia/tree/2e358ce975029ec97aba5994c17d4a2169c3b085/base/operators.jl#L381\" target=\"_blank\">operators.jl:381</a><li> sin<i>{T<:Number}</i>(::<b>AbstractArray{T<:Number,N}</b>) at <a href=\"https://github.com/JuliaLang/julia/tree/2e358ce975029ec97aba5994c17d4a2169c3b085/base/operators.jl#L383\" target=\"_blank\">operators.jl:383</a></ul>"
      ],
      "text/plain": [
       "# 11 methods for generic function \"sin\":\n",
       "sin(a::Complex{Float16}) at float16.jl:151\n",
       "sin(z::Complex{T<:Real}) at complex.jl:548\n",
       "sin(x::Float64) at math.jl:137\n",
       "sin(x::Float32) at math.jl:138\n",
       "sin(a::Float16) at float16.jl:150\n",
       "sin(x::BigFloat) at mpfr.jl:613\n",
       "sin(x::Real) at math.jl:139\n",
       "sin{Tv,Ti}(A::SparseMatrixCSC{Tv,Ti}) at sparse/sparsematrix.jl:648\n",
       "sin{T<:Number}(::AbstractArray{T<:Number,1}) at operators.jl:380\n",
       "sin{T<:Number}(::AbstractArray{T<:Number,2}) at operators.jl:381\n",
       "sin{T<:Number}(::AbstractArray{T<:Number,N}) at operators.jl:383"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods(sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "sin(x::<b>Real</b>) at <a href=\"https://github.com/JuliaLang/julia/tree/2e358ce975029ec97aba5994c17d4a2169c3b085/base/math.jl#L139\" target=\"_blank\">math.jl:139</a>"
      ],
      "text/plain": [
       "sin(x::Real) at math.jl:139"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@which sin(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "sin(x::<b>Float64</b>) at <a href=\"https://github.com/JuliaLang/julia/tree/2e358ce975029ec97aba5994c17d4a2169c3b085/base/math.jl#L137\" target=\"_blank\">math.jl:137</a>"
      ],
      "text/plain": [
       "sin(x::Float64) at math.jl:137"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@which sin(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just-in-time compilation (JIT)\n",
    "\n",
    "`Julia`'s efficiency results from its capabilities to infer the types of *all* variables within a function and then call LLVM to generate machine code at run-time. The key to writing performant `Julia` code is to be **type stable**, such that `Julia` is able to infer types of all variables and output of a function from the types of input arguments.\n",
    "\n",
    "Following figures and examples are taken from Arch D. Robinson's slides [*Introduction to Writing High Performance Julia*](https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxibG9uem9uaWNzfGd4OjMwZjI2YTYzNDNmY2UzMmE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| <img src=\"./benchmark/julia_toolchain.png\" alt=\"Julia toolchain\" style=\"width: 400px;\"/> | <img src=\"./benchmark/julia_introspect.png\" alt=\"Julia toolchain\" style=\"width: 500px;\"/> |\n",
    "|----------------------------------|------------------------------------|\n",
    "|||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bar (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function bar(x)\n",
    "    y = 1\n",
    "    x - y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Array{Any,1}:\n",
       " :($(Expr(:lambda, Any[:x], Any[Any[Any[:x,:Any,0],Any[:y,:Any,18]],Any[],0,Any[]], :(begin  # In[9], line 2:\n",
       "        y = 1 # In[9], line 3:\n",
       "        return x - y\n",
       "    end))))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@code_lowered bar(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables:\n",
      "  x::Int64\n",
      "  y::Int64\n",
      "\n",
      "Body:\n",
      "  begin  # In[9], line 2:\n",
      "      y = 1 # In[9], line 3:\n",
      "      return (Base.box)(Int64,(Base.sub_int)(x::Int64,y::Int64))\n",
      "  end::Int64\n"
     ]
    }
   ],
   "source": [
    "@code_warntype bar(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "define i64 @julia_bar_22137(i64) {\n",
      "top:\n",
      "  %1 = add i64 %0, -1\n",
      "  ret i64 %1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "@code_llvm bar(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t.section\t__TEXT,__text,regular,pure_instructions\n",
      "Filename: In[9]\n",
      "Source line: 3\n",
      "\tpushq\t%rbp\n",
      "\tmovq\t%rsp, %rbp\n",
      "Source line: 3\n",
      "\tleaq\t-1(%rdi), %rax\n",
      "\tpopq\t%rbp\n",
      "\tret\n"
     ]
    }
   ],
   "source": [
    "@code_native bar(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling Julia code\n",
    "\n",
    "Julia has several built-in tools for profiling. The `@time` marco outputs run time and heap allocation. The first time a funciton is called, Julia tries to compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.003664 seconds (1.47 k allocations: 79.237 KB)\n"
     ]
    }
   ],
   "source": [
    "function triple(a)\n",
    "    i = 1\n",
    "    n = length(a)\n",
    "    while i <= n\n",
    "        a[i] *= 3\n",
    "        i += 1\n",
    "    end\n",
    "end\n",
    "\n",
    "a = rand(10000)\n",
    "@time triple(a) # include compile time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000016 seconds (4 allocations: 160 bytes)\n"
     ]
    }
   ],
   "source": [
    "@time triple(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Profile` module gives line by line profile results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Count File                       Function                                 Line\n",
      "    83 ....4/IJulia/src/IJulia.jl eventloop                                 138\n",
      "    83 .../src/execute_request.jl execute_request                           164\n",
      "    54 In[14]                     triple                                      5\n",
      "    28 In[14]                     triple                                      6\n",
      "    83 loading.jl                 include_string                            282\n",
      "    82 profile.jl                 anonymous                                  16\n",
      "    83 task.jl                    anonymous                                 447\n"
     ]
    }
   ],
   "source": [
    "a = rand(100000000)\n",
    "Profile.clear()\n",
    "@profile triple(a)\n",
    "Profile.print(format=:flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed memory profiling requires a detour. First let's write a script `bar.jl`, which contains the workload function `tally` and a wrapper for profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function tally(x)\n",
      "    s = 0\n",
      "    for v in x\n",
      "        s += v\n",
      "    end\n",
      "    s\n",
      "end\n",
      "\n",
      "# call workload from wrapper to avoid misattribution bug\n",
      "function wrapper()\n",
      "    y = rand(1000)\n",
      "    # force compilation\n",
      "    println(tally(y))\n",
      "    # clear allocation counters\n",
      "    Profile.clear_malloc_data()\n",
      "    # run compiled workload\n",
      "    println(tally(y))\n",
      "end\n",
      "\n",
      "wrapper()\n"
     ]
    }
   ],
   "source": [
    ";cat benchmark/bar.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, in terminal, we run the script with `--track-allocation=user` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496.7690068613648\n",
      "496.7690068613648\n"
     ]
    }
   ],
   "source": [
    ";julia --track-allocation=user benchmark/bar.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The profiler outputs a file `bar.jl.mem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        - function tally(x)\n",
      "        0     s = 0\n",
      "        0     for v in x\n",
      "    32000         s += v\n",
      "        -     end\n",
      "        0     s\n",
      "        - end\n",
      "        - \n",
      "        - # call workload from wrapper to avoid misattribution bug\n",
      "        - function wrapper()\n",
      "        0     y = rand(1000)\n",
      "        -     # force compilation\n",
      "        0     println(tally(y))\n",
      "        -     # clear allocation counters\n",
      "        0     Profile.clear_malloc_data()\n",
      "        -     # run compiled workload\n",
      "      592     println(tally(y))\n",
      "        - end\n",
      "        - \n",
      "        - wrapper()\n",
      "        - \n"
     ]
    }
   ],
   "source": [
    ";cat benchmark/bar.jl.mem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the `tally` function type stable? How to diagnose and fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables:\n",
      "  x::Array{Float64,1}\n",
      "  s::Float64\n",
      "  #s52::Int64\n",
      "  v::Float64\n",
      "\n",
      "Body:\n",
      "  begin  # In[20], line 2:\n",
      "      s = (Base.box)(Float64,(Base.sitofp)(Float64,0)) # In[20], line 3:\n",
      "      #s52 = 1\n",
      "      GenSym(2) = (Base.arraylen)(x::Array{Float64,1})::Int64\n",
      "      unless (Base.box)(Base.Bool,(Base.not_int)(#s52::Int64 === (Base.box)(Base.Int,(Base.add_int)(GenSym(2),1))::Bool)) goto 1\n",
      "      2: \n",
      "      GenSym(4) = (Base.arrayref)(x::Array{Float64,1},#s52::Int64)::Float64\n",
      "      GenSym(5) = (Base.box)(Base.Int,(Base.add_int)(#s52::Int64,1))\n",
      "      v = GenSym(4)\n",
      "      #s52 = GenSym(5) # In[20], line 4:\n",
      "      s = (Base.box)(Base.Float64,(Base.add_float)(s::Float64,v::Float64))\n",
      "      3: \n",
      "      GenSym(3) = (Base.arraylen)(x::Array{Float64,1})::Int64\n",
      "      unless (Base.box)(Base.Bool,(Base.not_int)((Base.box)(Base.Bool,(Base.not_int)(#s52::Int64 === (Base.box)(Base.Int,(Base.add_int)(GenSym(3),1))::Bool)))) goto 2\n",
      "      1: \n",
      "      0:  # In[20], line 6:\n",
      "      return s::Float64\n",
      "  end::Float64\n"
     ]
    }
   ],
   "source": [
    "function tally(x)\n",
    "    s = zero(eltype(x))\n",
    "    for v in x\n",
    "        s += v\n",
    "    end\n",
    "    s\n",
    "end\n",
    "\n",
    "@code_warntype tally(rand(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization in Julia\n",
    "\n",
    "Many examples below involves numerical optimization, a key task in statistical estimation and inference. `Julia` offers various tools for optimization. The [JuliaOpt](http://www.juliaopt.org) organization is a collection of packages for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./benchmark/optimization_flowchart.png\" alt=\"Optimization flowchart\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE of Gamma distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use MLE of Gamma distribution to illustrate some rudiments of `Julia` programming. \n",
    "\n",
    "Let $x_1,\\ldots,x_m$ be a random sample from the gamma density\n",
    "\\begin{eqnarray*}\n",
    "f(x) & = & \\Gamma(\\alpha)^{-1} \\beta^{\\alpha} x^{\\alpha-1} e^{-\\beta x}\n",
    "\\end{eqnarray*}\n",
    "on $(0,\\infty)$. The loglikelihood function is\n",
    "\\begin{eqnarray*}\n",
    "    L(\\alpha, \\beta) = m [- \\ln \\Gamma(\\alpha) + \\alpha \\ln \\beta + (\\alpha - 1)\\overline{\\ln x} - \\beta \\bar x],\n",
    "\\end{eqnarray*}\n",
    "where $\\overline{x} = \\frac{1}{m} \\sum_{i=1}^m x_i$ and \n",
    "$\\overline{\\ln x} = \\frac{1}{m} \\sum_{i=1}^m \\ln x_i$. Following function \n",
    "evaluates the log-pdf of one data point `x` at parameter `α` and `β`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gamma_logpdf (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_logpdf(x::Real, α::Real, β::Real) = \n",
    "    - lgamma(α) + α * log(β) + (α - 1) * log(x) - β * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of `Julia`’s other strengths is its ability to infer the types of passed variables. To achieve the best computational efficiency, this information can be directly supplied by type annotation. Thus, the declaration `x::Real, α::Real, β::Real` alerts `Julia` to the fact that the data `x` and the parameters `α` and `β` must be a subtype of the abstract type `Real`. Mastery of type system allows one tap into `Julia`'s powerful multiple dispatch mechanism that dynamically dispatches functions according to the types of all arguments.\n",
    "\n",
    "`Julia` possesses a rich set of base functions. Our displayed code invokes the traditional scalar functions `log` and `lgamma`. Scalar functions can also act on vector arguments entry by entry. For example, if `x` is a vector, then `log(x)` delivers the natural logarithm of each entry of `x`. `Julia` documentation has a more complete [list](http://docs.julialang.org/en/release-0.4/manual/arrays/?highlight=vectorize#vectorized-operators-and-functions) of vectorized functions and operators. Scalar function not in the list can be easily vectorized. For example, to evaluate pdf or logpdf at a vector of observations, we can simply use the [`comprehension`](http://docs.julialang.org/en/release-0.4/manual/arrays/#comprehensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Any,1}:\n",
       " -0.768448\n",
       " -0.940515\n",
       " -0.673959\n",
       " -0.395453\n",
       " -0.313244"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srand(123) # set seed for rng\n",
    "x = rand(5)\n",
    "[gamma_logpdf(xi, 1.0, 1.0) for xi in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or [`map`](http://docs.julialang.org/en/release-0.4/stdlib/collections/#Base.map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " -0.768448\n",
       " -0.940515\n",
       " -0.673959\n",
       " -0.395453\n",
       " -0.313244"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(xi -> gamma_logpdf(xi, 1.0, 1.0), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduction functions such as `sum`, `mean` and `norm` also act on vector arguments but return a scalar. `Julia` provides several convenient ways to construct reduction function. To evaluate the log-likelihood of a random sample, we can use [`sum`](http://docs.julialang.org/en/release-0.4/stdlib/collections/#Base.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0916184386224517"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_logpdf(x::Vector, α::Real, β::Real) = \n",
    "    sum(xi -> gamma_logpdf(xi, α, β), x)\n",
    "gamma_logpdf(x, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or the more generic [`mapreduce`](http://docs.julialang.org/en/release-0.4/stdlib/collections/#Base.mapreduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0916184386224517"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_logpdf(x::Vector, α::Real, β::Real) = \n",
    "    mapreduce(xi -> gamma_logpdf(xi, α, β), +, x)\n",
    "gamma_logpdf(x, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "or the vecterized code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0916184386224517"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gamma_logpdf(x::Vector, α::Real, β::Real)\n",
    "    m = length(x)\n",
    "    avg = mean(x)\n",
    "    logavg = sum(log, x) / m\n",
    "    m * (- lgamma(α) + α * log(β) + (α - 1) * logavg - β * avg)\n",
    "end\n",
    "gamma_logpdf(x, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the function definition for evaluating pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gamma_pdf (generic function with 1 method)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_pdf(x, α, β) = exp(gamma_logpdf(x, α, β))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "works for both scalar and vector input x because of the multiple dispatch of `gamma_logpdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46373237313439586"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_pdf(x[1], 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04542837184470604"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma_pdf(x, 1.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization often invokes taking derivatives of the objective function. The `ForwardDiff` package implements automatic differentiation. For example, to compute the derivative and Hessian of the log-likelihood with data `x` at `α=1.0` and `β=1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.0782854\n",
       " 1.90838  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ForwardDiff\n",
    "ForwardDiff.gradient(r -> gamma_logpdf(x, r...), [1.0; 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2x2 Array{Float64,2}:\n",
       " -8.22467   5.0\n",
       "  5.0      -5.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ForwardDiff.hessian(r -> gamma_logpdf(x, r...), [1.0; 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the score function equal to \n",
    "${\\bf 0}$ implies that $\\beta = \\alpha / \\bar x$ is the maximizer \n",
    "of the loglikelihood $L(\\alpha,\\beta)$ for $\\alpha$ fixed.\n",
    "Substituting this value of $\\beta$ in the loglikelihood reduces \n",
    "maximum likelihood estimation to optimization of the profile loglikelihood\n",
    "\\begin{eqnarray}\n",
    "L(\\alpha) = m [\\alpha \\ln \\alpha - \\alpha \\ln \\overline{x} - \\ln \\Gamma(\\alpha) + (\\alpha-1) \\overline{\\ln x} - \\alpha].\n",
    "\\end{eqnarray}\n",
    "The stationarity equation\n",
    "\\begin{eqnarray}\n",
    "0 = m \\left( \\ln \\alpha -  \\ln \\overline{x} -  \\psi(\\alpha) + \\overline{\\ln x} \\, \\right)\n",
    "\\end{eqnarray}\n",
    "is ripe for solution by Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mle_gamma (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mle_gamma(x::Vector)\n",
    "    avg = mean(x)\n",
    "    d = log(avg) - sum(log, x) / length(x)\n",
    "    α = fzero(r -> log(r) - digamma(r) - d, 1e-6, Inf)\n",
    "    β = α / avg\n",
    "    return (α, β)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a test example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameter values:\n",
      "α = 3.8422383759828493, β = 4.7025750035759355\n",
      "Estimator by MLE:\n",
      "  0.260216 seconds (428.21 k allocations: 19.666 MB, 2.39% gc time)\n",
      "α̂ = 3.646663250650827, β̂ = 4.859696892325012\n",
      "L(α̂, β̂) = -3548.969909540808\n",
      "Estimator by MLE (Distribution package):\n",
      "Distributions.Gamma{Float64}(α=3.646663250650781, θ=4.8596968923250845)\n",
      "  0.107959 seconds (96.33 k allocations: 4.620 MB)\n"
     ]
    }
   ],
   "source": [
    "using Distributions, Roots\n",
    "\n",
    "srand(123)\n",
    "(n, p) = (1000, 2)\n",
    "(α, β) = 5.0 * rand(p)\n",
    "println(\"True parameter values:\")\n",
    "println(\"α = \", α, \", β = \", β)\n",
    "x = rand(Gamma(α, β), n)\n",
    "println(\"Estimator by MLE:\")\n",
    "@time (α, β) = mle_gamma(x)\n",
    "println(\"α̂ = \", α, \", β̂ = \", 1.0 / β)\n",
    "println(\"L(α̂, β̂) = \", gamma_logpdf(x, α, β))\n",
    "println(\"Estimator by MLE (Distribution package):\")\n",
    "@time println(fit_mle(Gamma, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try to use a generic nonlinear solver to find the MLE. We use the modelling tool [`JuMP.jl`](https://github.com/JuliaOpt/JuMP.jl) package, which support a variety of backend solvers such as `Ipopt`, `NLopt`, and `KNITRO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JuMP.UserFunctionEvaluator(myf,(anonymous function),2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using JuMP, Ipopt, NLopt, KNITRO\n",
    "\n",
    "myf(a, b) = gamma_logpdf(x, a, b)\n",
    "JuMP.register(:myf, 2, myf, autodiff=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max (nonlinear expression)\n",
      "Subject to\n",
      " α ≥ 1.0e-6\n",
      " β ≥ 1.0e-6\n",
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit http://projects.coin-or.org/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "This is Ipopt version 3.12.4, running with linear solver mumps.\n",
      "NOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:        0\n",
      "Number of nonzeros in inequality constraint Jacobian.:        0\n",
      "Number of nonzeros in Lagrangian Hessian.............:        0\n",
      "\n",
      "Total number of variables............................:        2\n",
      "                     variables with only lower bounds:        2\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        0\n",
      "Total number of inequality constraints...............:        0\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0  7.5268087e+03 0.00e+00 1.01e+00   0.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "   1  6.7312928e+03 0.00e+00 2.28e-02  -1.6 2.29e-02    -  1.00e+00 1.00e+00f  1\n",
      "   2  6.5428048e+03 0.00e+00 2.53e-03  -3.9 9.24e-03    -  1.00e+00 1.00e+00f  1\n",
      "   3  5.7635102e+03 0.00e+00 1.35e-03  -5.3 2.81e-02    -  1.00e+00 1.00e+00f  1\n",
      "   4  5.4933325e+03 0.00e+00 1.04e-03  -6.7 3.69e-02    -  1.00e+00 3.71e-01f  2\n",
      "   5  5.0209025e+03 0.00e+00 4.94e-04  -7.2 6.51e-02    -  1.00e+00 1.00e+00f  1\n",
      "   6  5.0415228e+03 0.00e+00 1.33e-03  -4.9 2.09e-01    -  1.00e+00 1.00e+00f  1\n",
      "   7  4.6897552e+03 0.00e+00 1.01e-03  -4.9 4.28e-02    -  1.00e+00 1.00e+00f  1\n",
      "   8  4.2657944e+03 0.00e+00 1.43e-04  -5.3 1.90e-01    -  1.00e+00 3.77e-01f  2\n",
      "   9  4.1707516e+03 0.00e+00 6.99e-04  -7.4 9.99e-02    -  1.00e+00 1.00e+00f  1\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "  10  3.9901442e+03 0.00e+00 1.97e-04  -5.8 2.11e-01    -  1.00e+00 1.00e+00f  1\n",
      "  11  3.8142105e+03 0.00e+00 1.29e-04  -5.9 3.78e-01    -  1.00e+00 1.00e+00f  1\n",
      "  12  3.6881257e+03 0.00e+00 6.50e-05  -6.3 4.85e-01    -  1.00e+00 1.00e+00f  1\n",
      "  13  3.6052992e+03 0.00e+00 3.73e-05  -6.6 5.96e-01    -  1.00e+00 1.00e+00f  1\n",
      "  14  3.5644366e+03 0.00e+00 1.62e-05  -7.0 6.08e-01    -  1.00e+00 1.00e+00f  1\n",
      "  15  3.5511904e+03 0.00e+00 5.64e-06  -7.7 4.78e-01    -  1.00e+00 1.00e+00f  1\n",
      "  16  3.5493051e+03 0.00e+00 2.12e-05  -8.8 2.54e-01    -  1.00e+00 1.00e+00f  1\n",
      "  17  3.5490652e+03 0.00e+00 1.25e-05  -7.5 5.33e+01    -  1.00e+00 9.77e-04f 11\n",
      "  18  3.5489700e+03 0.00e+00 3.54e-07 -11.3 1.00e-02    -  1.00e+00 1.00e+00f  1\n",
      "  19  3.5489699e+03 0.00e+00 2.43e-09 -11.3 1.36e-03    -  1.00e+00 1.00e+00f  1\n",
      "\n",
      "Number of Iterations....: 19\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:   3.5490071986838301e-04    3.5489699098311726e+03\n",
      "Dual infeasibility......:   2.4319353605387396e-09    2.4319098085817403e-02\n",
      "Constraint violation....:   0.0000000000000000e+00    0.0000000000000000e+00\n",
      "Complementarity.........:   5.0001944741929743e-12    5.0001419379470412e-05\n",
      "Overall NLP error.......:   2.4319353605387396e-09    2.4319098085817403e-02\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 44\n",
      "Number of objective gradient evaluations             = 20\n",
      "Number of equality constraint evaluations            = 0\n",
      "Number of inequality constraint evaluations          = 0\n",
      "Number of equality constraint Jacobian evaluations   = 0\n",
      "Number of inequality constraint Jacobian evaluations = 0\n",
      "Number of Lagrangian Hessian evaluations             = 0\n",
      "Total CPU secs in IPOPT (w/o function evaluations)   =      0.076\n",
      "Total CPU secs in NLP function evaluations           =      0.023\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      "  6.518558 seconds (9.47 M allocations: 410.156 MB, 1.69% gc time)\n",
      "Objective value: -3548.9699098311726\n",
      "α = 3.6465449117377466\n",
      "β = 4.859861203560224\n"
     ]
    }
   ],
   "source": [
    "m = Model(solver = IpoptSolver())\n",
    "#m = Model(solver = NLoptSolver(algorithm=:LD_MMA))\n",
    "#m = Model(solver = KnitroSolver())\n",
    "@variable(m, α >= 1e-6)\n",
    "@variable(m, β >= 1e-6)\n",
    "@NLobjective(m, Max, myf(α, β))\n",
    "\n",
    "print(m)\n",
    "@time status = solve(m)\n",
    "\n",
    "println(\"Objective value: \", getobjectivevalue(m))\n",
    "println(\"α = \", getvalue(α))\n",
    "println(\"β = \", 1 / getvalue(β))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE of Dirichlet distribution\n",
    "\n",
    "The Dirichlet distribution is used to model \n",
    "random proportions. It has probability density\n",
    "$$\n",
    "    \\frac{\\Gamma(\\theta_1 + \\cdots + \\theta_r)}{\\Gamma(\\theta_1) \\cdots \\Gamma(\\theta_r)} \\prod_{i=1}^r x_i^{\\theta_i - 1}\n",
    "$$\n",
    "on the unit simplex \n",
    "$\\{\\bx = (x_1,\\ldots,x_r)^t : x_1 > 0,\\ldots,x_r > 0 , \\sum_{i=1}^r x_i = 1 \\}$\n",
    "endowed with the uniform measure.  The Beta distribution is the\n",
    "special case $r=2$. \n",
    "\n",
    "If $\\bx_{1},\\ldots,\\bx_{m}$ are randomly sampled vectors from the \n",
    "Dirichlet distribution, then their loglikelihood is\n",
    "$$\n",
    "L(\\btheta) = m \\ln \\Gamma\\Big(\\sum_{i=1}^r \\theta_i\\Big) - m \\sum_{i=1}^r \\ln \\Gamma(\\theta_i) + \\sum_{j=1}^m \\sum_{i=1}^r (\\theta_i - 1) \\ln x_{ji}.\n",
    "$$\n",
    "Except for the first term on the right, the parameters are separated. Fortunately, the function $\\ln \\Gamma(t)$ is convex.  Its derivative, the\n",
    "digamma function, is denoted by $\\psi(t)$. The minorization\n",
    "$$\n",
    "\\ln \\Gamma\\Big(\\sum_{i=1}^r \\theta_i\\Big)\n",
    "\\ge\\ln \\Gamma\\Big(\\sum_{i=1}^r \\theta_{ni} \\Big) + \\psi\\Big(\\sum_{i=1}^r \\theta_{ni}\\Big) \\sum_{i=1}^r (\\theta_i-\\theta_{ni})\n",
    "$$\n",
    "generates the surrogate function\n",
    "$$\n",
    "g(\\btheta \\mid \\btheta_{n}) = m \\ln \\Gamma\\Big(\\sum_{i=1}^r \\theta_{ni}\\Big) + m \\psi\\Big(\\sum_{i=1}^r \\theta_{ni}\\Big) \\sum_{i=1}^r (\\theta_i-\\theta_{ni}) - m \\sum_{i=1}^r \\ln \\Gamma(\\theta_i) + \\sum_{j=1}^m \\sum_{i=1}^r (\\theta_i - 1) \\ln x_{ji}.\n",
    "$$\n",
    "Owing to the presence of the terms $\\ln \\Gamma(\\theta_i)$, the\n",
    "maximization step is analytically intractable.  However, Julia can readily solve\n",
    "the stationarity equation for each $\\theta_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mle_dirichlet (generic function with 1 method)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mle_dirichlet(x::Matrix)\n",
    "    p = size(x, 1)\n",
    "    avglog = sum(log, x, 2) / size(x, 2)\n",
    "    (θ, old_θ) = (ones(p), ones(p))\n",
    "    for iteration = 1:500\n",
    "        c = digamma(sum(θ))\n",
    "        for i = 1:p\n",
    "            θ[i] = fzero(r -> avglog[i] + c - digamma(r), 1e-6, Inf)\n",
    "        end\n",
    "        if norm(θ - old_θ) < 1e-6\n",
    "            break\n",
    "        else\n",
    "            copy!(old_θ, θ)\n",
    "        end \n",
    "    end\n",
    "    return θ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True parameter value:\n",
      "θ = [3.3697934728403367,1.977265561675543,1.5662197790375931]\n",
      "MLE by MM:\n",
      "θ̂ = [3.4645199552069186,2.139077459653193,1.6336678149454296]\n",
      "MLE by Distribution package:\n",
      "θ̂ = [3.4645249501579634,2.1390802743658073,1.633669813514543]\n"
     ]
    }
   ],
   "source": [
    "(n, p) = (1000, 3)\n",
    "θ = 5.0rand(p)\n",
    "println(\"True parameter value:\")\n",
    "println(\"θ = \", θ)\n",
    "x = rand(Dirichlet(θ), n)\n",
    "println(\"MLE by MM:\")\n",
    "θ = mle_dirichlet(x)\n",
    "println(\"θ̂ = \", θ)\n",
    "println(\"MLE by Distribution package:\")\n",
    "println(\"θ̂ = \", fit_mle(Dirichlet, x).alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNMF and GPU Computing\n",
    "\n",
    "In this example, we implement the MM algorithm for the Lee and Seung's Nonnegative Matrix Factorization (NNMF) algorithm and explore the GPU computing in Julia.\n",
    "\n",
    "Many EM/MM algorithms are amenable to massively parallel computing using GPU. See examples in the paper [*Graphics Processing Units and High-Dimensional Optimization*](http://projecteuclid.org/download/pdfview_1/euclid.ss/1294167962)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUs are ubiquitous in modern computers. Following are typical computer systems in **2015**.\n",
    "\n",
    "| NVIDIA GPUs         | Tesla M40                            | GTX Titan                                 | GTX 980M                              |\n",
    "|---------------------|----------------------------------------|-----------------------------------------|--------------------------------------|\n",
    "|                     | ![Tesla M40](./nnmf/tesla_m40.jpg) | ![GTX 580](./nnmf/nvidia_gtx_titan.jpg)    | ![GT 650M](./nnmf/nvidia_gtx980m.png) |\n",
    "| Computers           | servers, cluster                       | desktop                                 | laptop                               |\n",
    "|                     | ![Server](./nnmf/gpu_server.jpg)       | ![Desktop](./nnmf/alienware-area51.png) | ![Laptop](./nnmf/macpro_inside.png)  |\n",
    "| Main usage          | scientific computing                   | daily work, gaming                      | daily work                           |\n",
    "| Memory              | 12GB                                    | 6GB                                   | 1GB                                  |\n",
    "| Memory bandwidth    | 288GB/sec                              | 288GB/sec                               | 160GB/sec                             |\n",
    "| Number of cores     | 3072                                    | 2688                                     | 1536                                  |\n",
    "| Processor clock     | 1.3GHz                                 | 0.837GHz                                  | 1.038GHz                               |\n",
    "| Peak DP performance | 213Gflops                              |                                         |                                      |\n",
    "| Peak SP performance | 6.84Tflops                            | 4.5Tflops                              | 3.189Tflops                            |\n",
    "| Release price       | \\$4000                                  | \\$999                                    | OEM                                  |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU architecture vs CPU architecture.  \n",
    "* GPUs contain 100s of processing cores on a single card; several cards can fit in a desktop PC  \n",
    "* Each core carries out the same operations in parallel on different input data -- single program, multiple data (SPMD) paradigm  \n",
    "* Extremely high arithmetic intensity *if* one can transfer the data onto and results off of the processors quickly\n",
    "\n",
    "| <img src=\"./nnmf/cpu_i7_die.png\" alt=\"i7 die\" style=\"width: 400px;\"/> | <img src=\"./nnmf/Fermi_Die.png\" alt=\"Fermi die\" style=\"width: 350px;\"/> |\n",
    "|----------------------------------|------------------------------------|\n",
    "| ![Einstein](./nnmf/einstein.png) | ![Rain man](./nnmf/rainman.png)    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to [Jupyter Notebook](./nnmf/nnmf.ipynb).\n",
    "\n",
    "Exercise: Re-implement the NNMF algorithm using the [ArrayFire.jl](https://github.com/JuliaComputing/ArrayFire.jl) package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCP Using Convex.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard convex problem classes like LP (linear programming), QP (quadratic programming), SOCP (second-order cone programming), SDP (semidefinite programming), and GP (geometric programming), are becoming a **technology**. I illustrate `Julia` solutions to some statistical applications.\n",
    "\n",
    "<img src=\"./convex.jl/convex-hierarchy.png\" alt=\"DCP Hierarchy\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like computer languages, getting familiar with **good** optimization softwares broadens the scope and scale of problems we are able to solve in statistics. Following table lists some of the best optimization softwares. Use of Gurobi, Mosek, and Knitro is highly recommended. \n",
    "\n",
    "|           |   | LP | MILP | SOCP |     MISOCP     | SDP | GP | NLP | MINLP |   | R | Matlab | Julia | Python |   | Cost |\n",
    "|:---------:|:-:|:--:|:----:|:----:|:--------------:|:---:|:--:|:---:|:-----:|:-:|:-:|:------:|:-----:|:------:|:-:|:----:|\n",
    "|   **modeling tools**   |   |    |      |      |                |     |    |     |       |   |   |        |       |        |   |      |\n",
    "|  JuMP.jl  |   |  ✓ |   ✓  |   ✓  |        ✓       |     |    |  ✓  |   ✓   |   |   |        |   ✓   |        |   |   O  |\n",
    "| Convex.jl |   |  ✓ |   ✓  |   ✓  |        ✓       |  ✓  |    |     |       |   |   |        |   ✓   |        |   |   O  |\n",
    "|    cvx    |   |  ✓ |   ✓  |   ✓  |        ✓       |  ✓  |  ✓ |     |       |   |   |    ✓   |       |    ✓   |   |   A  |\n",
    "|   **convex solvers** |   |    |      |      |                |     |    |     |       |   |   |        |       |        |   |      |\n",
    "|   Gurobi  |   |  ✓ |   ✓  |   ✓  |        ✓       |     |    |     |       |   | ✓ |    ✓   |   ✓   |    ✓   |   |   A  |\n",
    "|   Mosek   |   |  ✓ |   ✓  |   ✓  |        ✓       |  ✓  |  ✓ |  ✓  |       |   | ✓ |    ✓   |   ✓   |    ✓   |   |   A  |\n",
    "|   CPLEX   |   |  ✓ |   ✓  |   ✓  |        ✓       |     |    |     |       |   | ? |    ✓   |   ✓   |    ✓   |   |   A  |\n",
    "|    SCS    |   |  ✓ |      |   ✓  |                |  ✓  |    |     |       |   |   |    ✓   |   ✓   |    ✓   |   |   O  |\n",
    "|   SeDuMi  |   |  ✓ |      |   ✓  |                |  ✓  |  ? |     |       |   |   |    ✓   |       |        |   |   O  |\n",
    "|   SDPT3   |   |  ✓ |      |   ✓  |                |  ✓  |  ? |     |       |   |   |    ✓   |       |        |   |   O  |\n",
    "|   **NLP solvers**  |   |    |      |      |                |     |    |     |       |   |   |        |       |        |   |      |\n",
    "|   KNITRO  |   |  ✓ |   ✓  |      |                |     |    |  ✓  |   ✓   |   | ✓ |    ✓   |   ✓   |    ✓   |   |   $  |\n",
    "|   NLopt   |   |  ✓ |      |      |                |     |    |  ✓  |       |   |   |    ✓   |   ✓   |    ✓   |   |   O  |\n",
    "|   Ipopt   |   |  ✓ |      |      |                |     |    |  ✓  |       |   |   |    ✓   |   ✓   |    ✓   |   |   O  |\n",
    "\n",
    "* O: open source  \n",
    "* A: free academic license  \n",
    "* $: commercial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to [Jupyter Notebook](./convex.jl/convex.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.2",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "417px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
